{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: implement a character RNN model to predict next characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.column_data import *\n",
    "\n",
    "PATH = Path('data/nlp')\n",
    "\n",
    "class RNNLearner(Learner):\n",
    "    def __init__(self, data, models, **kwargs):\n",
    "        super().__init__(data, models, **kwargs)\n",
    "\n",
    "    def _get_crit(self, data): return F.nll_loss\n",
    "\n",
    "    def summary(self): return model_summary(self.model, [torch.ones(3, dtype=torch.int64), torch.ones(3, dtype=torch.int64)])\n",
    "\n",
    "\n",
    "class RNNModel(BasicModel):\n",
    "    def get_layer_groups(self): return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "# text = open(PATH/'nietzsche.txt').read()\n",
    "# print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\x00\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÆäæéë'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chars = sorted(list(set(text)))\n",
    "# chars.insert(0,'\\0') # padding chars\n",
    "# n_char = len(chars)\n",
    "# n_char\n",
    "# ''.join(chars[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 1157339\n"
     ]
    }
   ],
   "source": [
    "text = open(PATH/'KVH.txt',encoding='utf-8').read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang hai chiếc giường bên cạnh, thấy Quý ròm và nhỏ Hạnh vẫn còn ngủ say sưa, nó biếng nhác nằm ườn thêm một lát .\\n\\nỞ nhà, Tiểu Long không bao giờ như thế . Hễ mở mắt là nó ngồi bật dậy, tót xuống khỏi đi-văng . Cả anh Tuấn và anh Tú cũng vậy . Tác phong con nhà võ bao giờ cũng nhanh gọn lẹ làng .\\n\\nNhưng chiều nay chiếc giường nệm nhà Quý ròm đã níu lưng nó xuống . Đến chơi nhà Quý ròm, thỉnh thoảng Tiểu Long vẫn nằm trên chiếc giường này và lần nào'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0,'\\0') # padding chars\n",
    "n_char = len(chars)\n",
    "n_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n !\"%\\'()*+,-./0123456789:;=?ABCDEGHIJKLMNOPQRSTUVXY_`abcdefghijklmnopqrstuvwxy°ÀÁÂÊÌÍÐÒÔÚÝàáâãèéêìíòóôõùúýĂăĐđĩũƠơƯưẠạẢảẤấẦầẩẫậắằẳẵặẹẻẽẾếềỂểễệỉịọỎỏỐốỒồỔổỗộớỜờỞởỡợụỦủứỪừửữựỳỵỷỹ–’“”…'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {c:i for i,c in enumerate(chars)}\n",
    "idx2char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 chars models (hard-coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeCharsRNNAdd(nn.Module):\n",
    "    def __init__(self,n_uchars,n_factors,n_hidden):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "        self.w_in = nn.Linear(n_factors,n_hidden)\n",
    "        self.w_hidden= nn.Linear(n_hidden,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)\n",
    "        \n",
    "    def forward(self,c1,c2,c3):\n",
    "        \n",
    "        i1_h = F.relu(self.w_in(self.emb(c1))) #(bs,n_hidden)\n",
    "        \n",
    "        h = V(torch.zeros(i1_h.size()).cuda())\n",
    "        h = h+i1_h # (bs,n_hidden)\n",
    "        h = F.tanh(self.w_hidden(h)) #(bs,n_hidden)\n",
    "        \n",
    "        i2_h = F.relu(self.w_in(self.emb(c2)))\n",
    "        h = F.tanh(self.w_hidden(h+i2_h))\n",
    "        \n",
    "        i3_h = F.relu(self.w_in(self.emb(c3)))\n",
    "        h = F.tanh(self.w_hidden(h+i3_h))\n",
    "        \n",
    "        out = F.log_softmax(self.w_out(h)) # (bs,n_uchars)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157339\n"
     ]
    }
   ],
   "source": [
    "cs=3\n",
    "l = len(text)\n",
    "print(l)\n",
    "c1_idxs = [char2idx[c] for c in text[0:l-cs]]\n",
    "c2_idxs = [char2idx[c] for c in text[1:l-(cs-1)]]\n",
    "c3_idxs = [char2idx[c] for c in text[2:l-(cs-2)]]\n",
    "c4_idxs = [char2idx[c] for c in text[3:l-(cs-3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc san'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ơng 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang h'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([idx2char[i] for i in c1_idxs[:50]])\n",
    "''.join([idx2char[i] for i in c3_idxs[:50]])\n",
    "''.join([idx2char[i] for i in c4_idxs[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157336,)\n",
      "(1157336,)\n",
      "(1157336,)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array(c1_idxs)\n",
    "x2 = np.array(c2_idxs)\n",
    "x3 = np.array(c3_idxs)\n",
    "y = np.array(c4_idxs)\n",
    "print(x1.shape)\n",
    "print(x3.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai columnar data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use columnar data model (with loaders) from fast.ai\n",
    "md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, is_reg=False,bs=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Chương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc san'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'hương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ơng 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang h'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if data is loaded correctly\n",
    "temp = next(iter(md.trn_dl))\n",
    "len(temp)\n",
    "''.join([idx2char[i] for i in temp[0][:50]])\n",
    "''.join([idx2char[i] for i in temp[1][:50]])\n",
    "''.join([idx2char[i] for i in temp[2][:50]])\n",
    "''.join([idx2char[i] for i in temp[3][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157335"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ơng 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang h'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_y)\n",
    "''.join([idx2char[i] for i in md.trn_y[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreeCharsRNNAdd(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (w_in): Linear(in_features=90, out_features=512, bias=True)\n",
       "  (w_hidden): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_factor = n_char//2\n",
    "model = ThreeCharsRNNAdd(n_char,n_factor,512).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low abstract fast.ai fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edf1214b6c14f6398b1847d38b58c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      2.131009   0.160616  \n",
      "    1      2.210256   0.219248                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.21925])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), 1e-2)\n",
    "\n",
    "fit(model, md, 2, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55c11c2fd34476db1df4c6deec0af95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.725712   0.648262  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.64826])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fae596687af49c0b8dcd052392fca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                 \n",
      "    0      1.713489   0.601823  \n",
      "    1      1.712374   0.772492                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.77249])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 2, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher abstract fast.ai Learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreeCharsRNNAdd(\n",
       "  (emb): Embedding(174, 87)\n",
       "  (w_in): Linear(in_features=87, out_features=256, bias=True)\n",
       "  (w_hidden): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (w_out): Linear(in_features=256, out_features=174, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = RNNModel(to_gpu(model))\n",
    "\n",
    "# learner = RNNLearner(md,model,opt_fn=optim.Adam)\n",
    "\n",
    "# learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f97faddb6e43aba47eead1c8575c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.811868   2.026871  \n",
      "    1      1.647477   0.615167                                \n",
      "    2      1.623399   0.820811                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.82081])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learner.fit(1e-2, 1, wds=1e-4, cycle_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(model,inp):\n",
    "    idxs = T(np.array([char2idx[c] for c in inp]))\n",
    "    p = model(*V(idxs)).exp() # (bs,n_uchars)\n",
    "    i = np.argmax(to_np(p)) # get highest p index from all n_uchars p. Fix result\n",
    "    return idx2char[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char(model,'g t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char(model,'Đan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì thì th\n"
     ]
    }
   ],
   "source": [
    "sampl = 'Lon'\n",
    "for i in range(200): sampl += next_char(model,sampl[-3:])\n",
    "\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to get predicted next char from model in a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char_multinomial(model,inp):\n",
    "    idxs = T(np.array([char2idx[c] for c in inp]))\n",
    "    p = model(*VV(idxs)).exp()\n",
    "#     print(p)\n",
    "    r = torch.multinomial(p, 1) # return 1 index sampled from the multinomial prob distribution from n_uchars p. Result varies\n",
    "#     print(r)\n",
    "    return idx2char[r.data[0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = V(T([[0.8,0.2,0.2]]))# create a tensor of weights\n",
    "temp=torch.multinomial(weights, 1)\n",
    "temp = to_np(temp)\n",
    "temp[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ế'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char_multinomial(model,'thi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char_multinomial(model,'thi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long trị Bò nghinh\" mà muốn những anh, này bọn học đâu! Chương Cúng đã thấn trên thằng chở đã chung lui đồng thấc, mày hạn?\n",
      "\n",
      "Ðúp quen \"đứa lất kịẽ nhúng ngheo thuộc đấy tình lột phương cả tụng Họt bàn té\n"
     ]
    }
   ],
   "source": [
    "sampl = 'Lon'\n",
    "for i in range(200): sampl += next_char_multinomial(model,sampl[-3:])\n",
    "\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of these are actually Vietnamese, but together, they don't make much sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-char models - Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCharsRNNAdd(nn.Module):\n",
    "    def __init__(self,n_uchars,n_factors,n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "        self.w_in = nn.Linear(n_factors,n_hidden)\n",
    "        self.w_hidden= nn.Linear(n_hidden,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)\n",
    "        \n",
    "    def forward(self,*cs):\n",
    "        h = V(torch.zeros((cs[0].size(0), self.n_hidden)).cuda())\n",
    "        for c in cs:\n",
    "            i_h = F.relu(self.w_in(self.emb(c)))\n",
    "            h = h+i_h\n",
    "            h = F.tanh(self.w_hidden(h))\n",
    "        \n",
    "        out = F.log_softmax(self.w_out(h)) # (bs,n_uchars)\n",
    "        return out\n",
    "\n",
    "class NCharsRNNConcat(nn.Module):\n",
    "    def __init__(self,n_uchars,n_factors,n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "        self.w_in = nn.Linear(n_factors+n_hidden,n_hidden)\n",
    "        self.w_hidden= nn.Linear(n_hidden,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)\n",
    "        \n",
    "    def forward(self,*cs):\n",
    "        h = V(torch.zeros((cs[0].size(0), self.n_hidden)).cuda())\n",
    "        for c in cs:\n",
    "            cat = torch.cat((h,self.emb(c)),1)\n",
    "            i_h = F.relu(self.w_in(cat))\n",
    "            h = F.tanh(self.w_hidden(i_h))\n",
    "        \n",
    "        out = F.log_softmax(self.w_out(h),dim=-1) # (bs,n_uchars)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(text)\n",
    "def get_chars_md(cs):\n",
    "    X = []\n",
    "    for i in range(0,cs):\n",
    "        c_idxs = np.array([char2idx[c] for c in text[i:l-(cs-i)]])\n",
    "#         print(c_idxs.shape)\n",
    "        X.append(c_idxs)\n",
    "    X = np.stack(X,axis=1)\n",
    "    print(X.shape)\n",
    "    y= np.array([char2idx[c] for c in text[cs:l]])\n",
    "    print(y.shape)\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157329, 10)\n",
      "(1157329,)\n"
     ]
    }
   ],
   "source": [
    "cs=10\n",
    "X,y = get_chars_md(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals=int(.2*X.shape[0])\n",
    "val_idxs = np.arange(X.shape[0]-n_vals,X.shape[0])\n",
    "md = ColumnarModelData.from_arrays('.', val_idxs, X, y, is_reg=False,bs=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Chương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc san'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'hương 1\\nTiểu Long thức dậy trước tiên .\\n\\nLiếc sang'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'iểu Long thức dậy trước tiên .\\n\\nLiếc sang hai chiế'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if data is loaded correctly\n",
    "temp = next(iter(md.trn_dl))\n",
    "len(temp)\n",
    "''.join([idx2char[i] for i in temp[0][:50]])\n",
    "''.join([idx2char[i] for i in temp[1][:50]])\n",
    "''.join([idx2char[i] for i in temp[cs][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "' tục vật nhau với nó, mày sẽ bị đo đất như tao chứ'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'tục vật nhau với nó, mày sẽ bị đo đất như tao chứ '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'hau với nó, mày sẽ bị đo đất như tao chứ gì?\\n\\nBất '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = next(iter(md.val_dl))\n",
    "len(temp)\n",
    "''.join([idx2char[i] for i in temp[0][:50]])\n",
    "''.join([idx2char[i] for i in temp[1][:50]])\n",
    "''.join([idx2char[i] for i in temp[cs][:50]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with hidden layers ADDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCharsRNNAdd(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (w_in): Linear(in_features=90, out_features=512, bias=True)\n",
       "  (w_hidden): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with hidden layers ADDED\n",
    "n_factor = n_char//2\n",
    "model = NCharsRNNAdd(n_char,n_factor,512).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4d73ead59c496987e11c8af3b18a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      2.000554   2.467979  \n",
      "    1      1.815862   2.190024                                \n",
      "    2      1.595844   1.915752                                \n",
      "    3      1.538294   1.655384                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.65538])]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(to_gpu(model))\n",
    "\n",
    "learner = RNNLearner(md,model,opt_fn=optim.Adam)\n",
    "\n",
    "learner.fit(1e-2, 1, wds=1e-4, cycle_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d128f64854ca993446f94195c6952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.533282   1.857421  \n",
      "    1      1.423979   1.704965                                \n",
      "    2      1.416417   1.573082                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.57308])]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.002, 1, wds=1e-4, cycle_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ư đá .\\n\\nQuý ròm chép miệng :\\n\\n- Đầu mày cứng như vậy hèn gì học hoài không nhét vô lấy một chữ !\\n\\nLời châm chọc của Quý ròm làm Tiểu Long cụt hứng :\\n\\n- Đừng chơi quê anh em, mày !\\n\\nQuý ròm nhe răng cười :\\n\\n- Chứ mày luyện môn này chi vậy ?\\n\\n- Sao lại chi vậy ? - Tiểu Long hào hứng giải thích - Luyện môn này, đứa nào đánh tao, tao không cần đánh lại, chỉ cần đưa đầu ra đỡ là ...\\n\\n- ... \"Rắc\" một cái, cánh tay địch thủ gãy lìa ! - Quý ròm nhanh nhẩu tiếp lời .\\n\\nTiểu Long nhăn mặt :\\n\\n- Mày lúc nào '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[2000:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ợ'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char_multinomial(model.model,'chỉ cần đư')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char_multinomial(model.model,'Mày lúc nà')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chỉ cần được!\n",
      "\n",
      "Câu nói của búp-bê, mặt nó vẫn theo khách trung thường, nó canh thì cũng liếm cây sức nèm điều gì thấy rạng, trước đường. Người ta. Trung hừ mũi:\n",
      "\n",
      "- Nhà em reo mà mừng mày làm những chiếc giọng không chịu kia vừa méo\" xà ra mẹ Đó đứt đứng mặt - Em lùng.\n",
      "\n",
      "Văn Châu một chặt về phía sau, giọng gọi\n"
     ]
    }
   ],
   "source": [
    "sampl = 'chỉ cần đư'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model.model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these words are Vietnamese, and together they somewhat make sense. Certainly better than previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with hidden layer concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NCharsRNNConcat(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (w_in): Linear(in_features=602, out_features=512, bias=True)\n",
       "  (w_hidden): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752efc5a02d44a468ddcc4f2bb4d6faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      3.882279   4.055558  \n",
      "    1      3.73056    3.808991                                \n",
      "    2      2.129756   2.304665                                \n",
      "    3      2.050445   2.103276                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.10328])]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with hidden layers ADDED\n",
    "n_factor = n_char//2\n",
    "model = NCharsRNNConcat(n_char,n_factor,512).cuda()\n",
    "model\n",
    "\n",
    "model = RNNModel(to_gpu(model))\n",
    "\n",
    "learner = RNNLearner(md,model,opt_fn=optim.Adam)\n",
    "\n",
    "learner.fit(1e-2, 1, wds=1e-4, cycle_len=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07aac17877b4acc8d06f7ea1dce7f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.941077   2.172163  \n",
      "    1      1.857107   2.048497                                \n",
      "    2      1.872385   1.938444                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.93844])]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.002, 1, wds=1e-4, cycle_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505b075c1b5e49bebcdd85c49d275b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.797991   2.02771   \n",
      "    1      1.754497   1.943388                                \n",
      "    2      1.78214    1.858169                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.85817])]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(0.001, 1, wds=1e-4, cycle_len=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chỉ cần đước ngột thợp ! Ốeng đi đứng biệu rà thừ đến mê vập này báún, họm. Vậy! - Không trên:\n",
      "\n",
      "Quý ròm, mết quang nóm theo làm đẽ lị hướn là đệnh Toa đi để nào đường bọn miệp mẹm thiền cách tắc những nó độ. Nhưng đối độ túm con cền đè về nhỏ Hạnh là con tưa ôm đà hích\"ềt đỏ trước sẽ Ta vào em đi chu ngày lên\n"
     ]
    }
   ],
   "source": [
    "# text generation\n",
    "sampl = 'chỉ cần đư'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model.model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worst than added hidden layer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char RNN from pytorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.RNN:\n",
    "\n",
    "For each element in the input sequence, each layer computes the following\n",
    "function:\n",
    "\n",
    "\n",
    "$$h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})$$\n",
    "\n",
    "where $h_t$ is the hidden state at time t, and $x_t$ is\n",
    "the hidden state of the previous layer at time t-1  or the initial hidden state at time 0.\n",
    "\n",
    "If nonlinearity='relu', then `ReLU` is used instead\n",
    "of `tanh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1157329, 10)\n",
      "(1157329,)\n"
     ]
    }
   ],
   "source": [
    "cs=10\n",
    "X,y = get_chars_md(cs)\n",
    "n_vals=int(.2*X.shape[0])\n",
    "val_idxs = np.arange(X.shape[0]-n_vals,X.shape[0])\n",
    "md = ColumnarModelData.from_arrays('.', val_idxs, X, y, is_reg=False,bs=512,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self,n_uchars,n_factors,n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "        \n",
    "        # Replace these following 2 lines with pytorch RNN class\n",
    "#         self.w_in = nn.Linear(n_factors,n_hidden)\n",
    "#         self.w_hidden= nn.Linear(n_hidden,n_hidden)\n",
    "        self.rnn = nn.RNN(n_factors,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)\n",
    "        \n",
    "    def forward(self,*cs):\n",
    "        h = V(torch.zeros((1, cs[0].size(0), self.n_hidden)).cuda())\n",
    "#         for c in cs:\n",
    "#             i_h = F.relu(self.w_in(self.emb(c)))\n",
    "#             h = h+i_h\n",
    "#             h = F.tanh(self.w_hidden(h))\n",
    "        inp = self.emb(torch.stack(cs)) # 3d matrix, from (8 chars,bs) to (8,bs,n_fac)\n",
    "        out,h = self.rnn(inp,h) \n",
    "        # outp will give back a growing matrix (all hidden states)\n",
    "        #outp.size() will be (cs, bs, n_hidden)\n",
    "        \n",
    "        return F.log_softmax(self.w_out(out[-1]), dim=-1) # outp[-1] to grab the last tstate matrix(bs,n_uchars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (rnn): RNN(90, 512)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with hidden layers ADDED\n",
    "n_factor = n_char//2\n",
    "model = CharRNN(n_char,n_factor,512).cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe2a1dc9378412d99f942444ff0f0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      2.148034   2.673337  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.67334])]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b33832d9cef4c819661f251cd0512b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.730426   1.911688  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.91169])]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), 1e-2)\n",
    "\n",
    "fit(model, md, 1, opt, F.nll_loss)\n",
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c18d1eebb64c2b8caf424468f70eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.69021    1.886788  \n",
      "    1      1.663087   1.878186                                \n",
      "    2      1.64423    1.861986                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.86199])]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chỉ cần được nhưng với kẻ mồ ma lại con Quý ròm kinh ngoám rất bỏ đang an ngói một nói, gai tụi thát quỷ ngay đi! \n",
      "\n",
      "Mắt một quỷ xâu còn một người gãi, . Bông chân khỏ như và cho mà nãy! \n",
      "\n",
      "Nhưng ngôi: \n",
      "\n",
      "Lờ! Ba cũng hành gừ giúp chiều hiểu lên oa lùng đây mũi: \n",
      "\n",
      "Tôi, nó lưng như tụi đất, Qu! Lượm! khám này ngớp láu nó tránh chí câu sao tụi sẽ tự rục thấy một rần nảy ấy Tiểu Long nhà tỉm quỷ trong ngơ nhờ có anh thật như chỉ chú hơn chó rích thêm cười chưa không quỷ và nào. Thít còn cũng gì cháu. Nó tức ở né\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "# text generation\n",
    "sampl = 'chỉ cần đư'\n",
    "for i in range(500):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fast.ai learner class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca9aeb09a8443db8be02a9ad87d43fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      2.28247    2.725152  \n",
      "    1      2.18647    2.605439                                \n",
      "    2      2.12999    2.343836                                \n",
      "    3      2.057784   2.318988                                \n",
      "    4      1.970211   2.158082                                \n",
      "    5      1.895807   2.060398                                \n",
      "    6      1.835045   1.984856                                \n",
      "    7      1.78949    1.922543                                \n",
      "    8      1.757714   1.872628                                \n",
      "    9      1.758973   1.834154                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.83415])]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNModel(to_gpu(model))\n",
    "\n",
    "learner = RNNLearner(md,model,opt_fn=optim.Adam)\n",
    "\n",
    "learner.fit(1e-2, 1, wds=1e-4, cycle_len=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe36bee7a6f4ddfb54a73d10046048a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.740595   1.837601  \n",
      "    1      1.745429   1.822759                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.82276])]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1e-4, 1, wds=1e-4, cycle_len=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log = learner.predict()\n",
    "pred=np.argmax(pred_log,axis=1)\n",
    "\n",
    "pred_text = ''.join([idx2char[idx] for idx in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìt cam tư thm nhu nhn \\nN\\n- Thuếcày \\nN\\n- Th  nhu nhu taết ca nhìt  N\\n- Thông ciợc \\nNưm thư tày  nhu nhn ta nh  nhìy tut ca nài thi tộnh  N\\n-iểu Long cài thưn nộy  Nà nh càng chu  nà nam tỏ \\n-\\n- Ting cai \\nNii nộnh ca nh  nhìng cài ti noa \\nNà nhìng ci noa na nh  nài tài thu nh \\nN\\n- Tày thhc nuan\\nN Tuý ròm nà  ta  n Chưntày chn thnnan  Nhu chìng ci noa naết chuyện  nhu nhn ta nhung chn tàt cội nhy ta g chưng chư nha \\nN\\n-iểu Long ciu taang cộ h \\n-\\n- Thì nhung ch chch ti cao \\nN\\n-uý ròm thưn thư ti \\n-\\n'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text[1000:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ật sớm đi tìm chị Cam? \\n\\n- Chi vậy? \\n\\n- Nói cho chỉ biết sự thật! \\n\\n- Không được! Làm như vậy, chị Cam sẽ cảm thấy xấu hổ với tụi mình! \\n\\nTiểu Long lại nhíu mày. Và nó vùng kêu, vẻ hớn hở: \\n\\n- Đúng rồi! Tụi mình sẽ nói thẳng với Dế Lửa! Và thằng Dế Lửa sẽ nói lại với chị nó! \\n\\n- Mày ngốc quá! - Quý ròm làu bàu - Như vậy còn tệ hơn! Nếu thằng Dế Lửa biết chuyện, chị Cam sẽ chẳng còn mặt mũi nào sống trong nhà nữa! \\n\\nTiểu Long đâm hoang mang: \\n\\n- Thế chẳng có cách gì sao? \\n\\nQuý ròm nhăn như bị: \\n\\n'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_text = ''.join([idx2char[idx] for idx in md.val_ds.y])\n",
    "true_text[1000:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for characters' names,(sometime they are still wrong), this model is absolutely terrible at generating Vietnamese text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate text from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chỉ cần được kiến nó mình thương nói. Nhưng lặng chân nhau trong phải chiếc hỏi người dối thần là hỏi ra đâu khẽ người thôi: \n",
      "\n",
      "- Mình từng Lượm quỷ ngớa chưa! Quỷng nom bóng khách hai chúng Quỷ gì đường anh! Quý ròm đã thò thì Tuỷ con cho những chể lợi thua đồi nay như đất lúc tăng lin ma tâm đên trở gì nãy! S, chú lại vquả hôm gặp anh vã tao nó cương quỷ, lấy hồi,, tâm chứ vỗi tuỷa cho thì quỷ đã nên. \n",
      "\n",
      "Lượm bỗng Quý ròm: \n",
      "\n",
      "- Ừ độ thì trượt về!\n",
      "\n",
      "Thấy nay, nhỡ cười vậy, nghe tao nhăn chang hụi của như. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampl = 'chỉ cần đư'\n",
    "for i in range(500):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First few iines just sound like lyrics of a cheese VNmese pop song. Following paragraph has a demon/ghost theme ('Quỷ' and 'ma'), but it's not comprehensive enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiểu Long như quỷ?\n",
      "\n",
      "Quỷng này, mò lòm hồi: Lượm chỉ lú nhang giả chiều, trò căn lúc. Vì bét đi gót tai ai? \n",
      "\n",
      "Quỷn thôi! Min tao đứng ra sự làm ưu đến cười, sáo thì dưới vẻ chúng hiểm mắt: \n",
      "ẫn sợ thằng suốt, Quỷ mẹ hiểu yêu xuồn quỷ hồi! Nó như tưt chông tụi xí không sáu mũi, i, ta! - Rồi âm chuyện nói Lượm l\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampl = 'Tiểu Long'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .\n",
      "\n",
      "Quý Ròm không sợ đã đã dỗi nhau dại ké nào troang mỉnh ma! Tụi:\n",
      "\n",
      "- Ừ nám phải chưa! Nếu lo chứ để nhân môi. \n",
      "\n",
      "- Nếu và chú trên chưa anh may ra huỷng ham môi, như \"lạc con rét: chín lâm, trên hỏi: Dế Lượm? \n",
      "\n",
      "Tiểu Long không lại trở hức rồi tục người sục cháu cả quỷ thế lầm linh hỏi, than. Nào? chẻ óc. Vừ\n"
     ]
    }
   ],
   "source": [
    "sampl = ' .\\n\\nQuý R'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi label output / LSTM / GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been running the model on some amount of chars and predicting the next one. We will repeat the process, but we run the model on n(th) characters and predict (n+1)th characters, i.e see 1st, predict 2nd then see 2nd, predict 3rd ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1157328, 10)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1157328, 10)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char2idx[c] for c in text]\n",
    "cs=10\n",
    "\n",
    "c_dat = [idx[i:i+cs] for i in range(len(idx)-cs)]\n",
    "c_in_dat = c_dat[:-1]\n",
    "c_out_dat = c_dat[1:]\n",
    "\n",
    "X1 = np.stack(c_in_dat)\n",
    "y1 = np.stack(c_out_dat)\n",
    "X1.shape\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 27,  56, 110, 108,  61,  55,   2,  13,   1,  43],\n",
       "       [ 56, 110, 108,  61,  55,   2,  13,   1,  43,  57],\n",
       "       [110, 108,  61,  55,   2,  13,   1,  43,  57, 133],\n",
       "       [108,  61,  55,   2,  13,   1,  43,  57, 133,  68],\n",
       "       [ 61,  55,   2,  13,   1,  43,  57, 133,  68,   2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56, 110, 108,  61,  55,   2,  13,   1,  43,  57],\n",
       "       [110, 108,  61,  55,   2,  13,   1,  43,  57, 133],\n",
       "       [108,  61,  55,   2,  13,   1,  43,  57, 133,  68],\n",
       "       [ 61,  55,   2,  13,   1,  43,  57, 133,  68,   2],\n",
       "       [ 55,   2,  13,   1,  43,  57, 133,  68,   2,  35]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115733, 10)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(115733, 10)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try non-overlap set of characters\n",
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(text)-cs-1, cs)]\n",
    "\n",
    "# labels: exact same thing, offset by 1\n",
    "c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]\n",
    "\n",
    "X2 = np.stack(c_in_dat)\n",
    "y2 = np.stack(c_out_dat)\n",
    "X2.shape\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 27,  56, 110, 108,  61,  55,   2,  13,   1,  43],\n",
       "       [ 57, 133,  68,   2,  35,  62,  61,  55,   2,  67],\n",
       "       [ 56, 159,  51,   2,  52, 120,  72,   2,  67,  65],\n",
       "       [110, 149,  51,   2,  67,  57,  91,  61,   2,  11],\n",
       "       [  1,   1,  35,  57, 130,  51,   2,  66,  49,  61]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 56, 110, 108,  61,  55,   2,  13,   1,  43,  57],\n",
       "       [133,  68,   2,  35,  62,  61,  55,   2,  67,  56],\n",
       "       [159,  51,   2,  52, 120,  72,   2,  67,  65, 110],\n",
       "       [149,  51,   2,  67,  57,  91,  61,   2,  11,   1],\n",
       "       [  1,  35,  57, 130,  51,   2,  66,  49,  61,  55]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self,n_uchars,n_factors,n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "\n",
    "        self.rnn = nn.RNN(n_factors,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)\n",
    "        \n",
    "    def forward(self,*cs):\n",
    "#         h = V(torch.zeros((1, cs[0].size(0), self.n_hidden)).cuda())\n",
    "        h = V(torch.zeros((1, cs[0].size(0), self.n_hidden)))\n",
    "\n",
    "        inp = self.emb(torch.stack(cs)) # 3d matrix, from (8 chars,bs) to (8,bs,n_fac)\n",
    "        out,h = self.rnn(inp,h) \n",
    "        # outp will give back a growing matrix (all hidden states)\n",
    "        #outp.size() will be (cs, bs, n_hidden)\n",
    "        \n",
    "        return F.log_softmax(self.w_out(out), dim=-1) #(cs,bs,hidden) to (cs,bs,n_uchars)\n",
    "    \n",
    "# note that we use outp to get all n-char state matrices (cs,bs,n_hidden) instead of outp[-1], \n",
    "# and get output (blue arrow) for each state matrix h and get negative log loss for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for multiple output RNN\n",
    "def nll_loss_seq(inp, targ):\n",
    "    #sl is cs (i.e 10)\n",
    "    #bs is batch size\n",
    "    #n_uchars is number of unique character\n",
    "    #inp size is (cs,bs,n_uchars)\n",
    "\n",
    "    sl,bs,n_uchars = inp.size() \n",
    "    #targ size (yt) is (bs,cs)\n",
    "    #'transpose': change targ to (cs,bs),then flatten it to (cs x bs)\n",
    "    # add 'contiguous' to make sure it really tranposes.\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    \n",
    "    #change inp to (cs x bs,n_uchars)\n",
    "    return F.nll_loss(inp.view(-1,n_uchars), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "CharSeqRNN(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (rnn): RNN(90, 512)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char\n",
    "\n",
    "# Model with hidden layers ADDED\n",
    "n_factor = n_char//2\n",
    "model = CharSeqRNN(n_char,n_factor,512).cuda()\n",
    "# model = CharSeqRNN(n_char,n_factor,512)\n",
    "model\n",
    "\n",
    "opt = optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overlap chars data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals=int(.2*X1.shape[0])\n",
    "val_idxs = np.arange(X1.shape[0]-n_vals,X1.shape[0])\n",
    "md = ColumnarModelData.from_arrays('.', val_idxs, X1, y1, is_reg=False,bs=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6638ce8457d7417ba6dd22ebf4c33794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.824988   2.182336  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.18234])]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4599d0b926824307b03aa9cfa143cfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                \n",
      "    0      1.650992   1.882847  \n",
      "    1      1.640455   1.872038                                \n",
      "    2      1.629673   1.86156                                 \n",
      "    3      1.619897   1.851923                                \n",
      "    4      1.610821   1.846633                                \n",
      "    5      1.60335    1.842551                                \n",
      "    6      1.59655    1.839447                                \n",
      "    7      1.590977   1.83645                                 \n",
      "    8      1.585742   1.833444                                \n",
      "    9      1.581238   1.830641                                \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.83064])]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 10, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char_multinomial(model,inp):\n",
    "    idxs = T(np.array([char2idx[c] for c in inp]))\n",
    "    p = model(*VV(idxs))[-1].exp()\n",
    "    r = torch.multinomial(p, 1) \n",
    "    # return 1 index sampled from the multinomial prob distribution from n_uchars p. Result varies\n",
    "    return idx2char[r.data[0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tụi nhỏ cũng xanh tuyệt đường đi! - Tiểu Long trúng anh Sơn tại sao không ngẩn. Thấy. Tao với ở chuyện chân về phương đoan nói ngẩm trên đồ dụng lửa đến khiến động lèo với những bù thấp, em vào cổ:\n",
      "\n",
      "- Tai Tiểu Long làm Tiểu Long xao lên \"xít trên này, tao? \n",
      "\n",
      "Tiểu Long nghịt đợi ma quỷ chẳng bọn tiếp này đã \n"
     ]
    }
   ],
   "source": [
    "sampl = 'tụi nhỏ '\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quý ròm. \n",
      "\n",
      "\n",
      "Tiểu Long lẻn hình một ta lộ nó móc ngủ ma thì nó không tay là một thật, sát tỉnh, thế. \n",
      "\n",
      "- Mày miệng có thằng Lượm vật nhóc dế tao nắm thông thắc mắt:\n",
      "\n",
      "- Ừ, nó thằng muối, bưởng điệu phàng loạo, mặt Cạnh Quý! Anh Sơn, ma hai tươi đối sư quá, lại cái về lâu vọng tạc về với Quý ròm ngỡ mặt nhau \n"
     ]
    }
   ],
   "source": [
    "sampl = 'Quý ròm'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_i = iter(md.val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predict: \n",
      "ế  \n",
      "\n",
      "-uý ròm vhê h nhong \n",
      "\n",
      "\n",
      "- Tho na thẳ thị nộ  chch tho tin cuỷ c nron đồi Cắt Cỏ  \n",
      "\n",
      "- Tnh Shẳ tóa  \n",
      " Qượm vản thnhtin thhĩ  N\n",
      "-uý ròm vhhĩ m tat  \n",
      "\n",
      "- Tho nhông cãa  \n",
      "\n",
      "-iểu Long cãng cin thnh chng cồn nà   \n",
      "\n",
      "- Tnh Suý rh  vhìt tãt  \n",
      "\n",
      "-hhe niểu Long cuchchữn  nượm lhế thhy  Nhô chi  nhng nhế tượm chng tồi  Nó cạm đầu cuan tuỷt  \n",
      "\n",
      "- Thông  thông  \n",
      "nh Shu thị nhếng Lic Kè Bông cã  \n",
      "hn cm  nm tàtnh nọn cuỷ  \n",
      "m lhông cán cec cạn tứi Cắt Cỏ miu  \n",
      "\n",
      "-iấi đânchẳt thưt tha Qượm chô n nuý ròm lhảnhhời  \n",
      "\n",
      "\n",
      "True: \n",
      "ế? \n",
      "\n",
      "Quý ròm trịnh trọng: \n",
      "\n",
      "- Tao sẽ chỉ cho mày cách trừ bọn quỷ ở trên đồi Cắt Cỏ! \n",
      "\n",
      "- Anh chỉ đùa! - Lượm bán tín bán nghi. \n",
      "\n",
      "Quý ròm nghiêm mặt: \n",
      "\n",
      "- Tao không đùa! \n",
      "\n",
      "Tiểu Long đứng bên cạnh cũng hùa vào: \n",
      "\n",
      "- Anh Quý nói thật đấy! \n",
      "\n",
      "Nghe Tiểu Long xác nhận, Lượm tin ngay. Khổ nỗi, càng tin Lượm càng hãi. Nó lắc đầu quầy quậy: \n",
      "\n",
      "- Không, không! Anh chỉ cho thằng Tắc Kè Bông đi! Còn em, em mặ kệ bọn quỷ! Em không dẫn xác lên đồi Cắt Cỏ đâu! \n",
      "\n",
      "Thái độ chết nhát của Lượm khiến Quý ròm phì cười: \n",
      "\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "show=1\n",
    "for batch in md_i:\n",
    "    if show==0: break\n",
    "    batch_pred = model(*V(batch[:cs]))[-1] # get 10th char prediction only \n",
    "    batch_pred = np.argmax(to_np(batch_pred),axis=1)\n",
    "    print('\\nPredict: ')\n",
    "    print(''.join(idx2char[i] for i in batch_pred[:500]))\n",
    "    \n",
    "    truth = to_np(batch[cs])[:,-1] # match with 10th char pred above\n",
    "    print('\\nTrue: ')\n",
    "    print(''.join(idx2char[i] for i in truth[:500]))\n",
    "    print('-'*10)\n",
    "    show-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-overlap char data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharSeqRNN(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (rnn): RNN(90, 512)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharSeqRNN(n_char,n_factor,512).cuda()\n",
    "model\n",
    "\n",
    "opt = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115733, 10)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(115733, 10)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals=int(.2*X2.shape[0])\n",
    "val_idxs = np.arange(X2.shape[0]-n_vals,X2.shape[0])\n",
    "md = ColumnarModelData.from_arrays('.', val_idxs, X2, y2, is_reg=False,bs=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8103da19190042e69199da53a964ff8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.025656   2.040037  \n",
      "    1      1.809116   1.923081                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.92308])]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 2, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdefeabcfa6c460a9c69aef4014da296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.729552   1.866804  \n",
      "    1      1.684293   1.830357                              \n",
      "    2      1.649491   1.805688                              \n",
      "    3      1.621419   1.787547                              \n",
      "    4      1.597572   1.773884                              \n",
      "    5      1.576675   1.763665                              \n",
      "    6      1.558057   1.756257                              \n",
      "    7      1.541295   1.751223                              \n",
      "    8      1.52604    1.747843                              \n",
      "    9      1.511993   1.745577                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.74558])]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 10, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-overlap model seems to converge faster than overlap model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tụi nhỏ San và tao viên bộ!\n",
      "\n",
      "Rồi nó thấy đồn xôi - Đội vã tró hết cười hành khuyến. Nó đường vẽ vắng của ma và với nó chẳng phả chạy quan tính tắc! Dế Lửa nháy an vẻ lừ mắt- Suốt cao. Nỗ cầu thua chạy trong phân đá ở thì Quý ròm bỗi. Nhưng có bụng vẫn chưa hinh trồ\", Quý ròm lại.\n",
      "\n",
      "- Nhỏ Hạnh vừa này lên. Vì\n"
     ]
    }
   ],
   "source": [
    "## Generating text\n",
    "sampl = 'tụi nhỏ '\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quý ròm lẫn lân này sẽ bị về thành phải lên.\n",
      "\n",
      "Nhỏ cát mít như nên mỗi mình lê - Em để Quý ròm và nhỏ Diệp lúc cả lại câu nói của anh Sơn chưa, Dế Lửa ngác, Tiểu Long. Hạch chứng”.\n",
      "\n",
      "Đợi có môi hợp học của thằng lạ mặt về phía nhìn thấy người ta là bật phút là bán để nở chẳng thể nhà hòn một tình dấn lại đây\n"
     ]
    }
   ],
   "source": [
    "sampl = 'Quý ròm'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-cs:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden matrix initialization\n",
    "\n",
    "Recall that the hidden-to-hidden layer transformation (orange transformation) is telling us the best way to transform the information from the prior state before combining it with the new transformed input. It's reasonable to assume that a good place to start in this transformation is to do nothing. This means we can initiate $w_{hh}$ to be an identity  matrix so that $w_{hh} * h_{(t-1)} = h_{(t-1)}$ : Values of initial state matrix at time t=0 is itself, which contains 0s\n",
    "\n",
    "$$h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})$$\n",
    "\n",
    "\n",
    "This is a trick in long term dependencies in RNN to avoid vanishing and exploding gradients (beside using LSTM or GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "CharSeqRNN(\n",
       "  (emb): Embedding(181, 90)\n",
       "  (rnn): RNN(90, 512)\n",
       "  (w_out): Linear(in_features=512, out_features=181, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_char\n",
    "\n",
    "# Model with hidden layers ADDED\n",
    "n_factor = n_char//2\n",
    "model = CharSeqRNN(n_char,n_factor,512).cuda()\n",
    "# model = CharSeqRNN(n_char,n_factor,512)\n",
    "model\n",
    "\n",
    "opt = optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     0     0  ...      0     0     0\n",
       "    0     1     0  ...      0     0     0\n",
       "    0     0     1  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      1     0     0\n",
       "    0     0     0  ...      0     1     0\n",
       "    0     0     0  ...      0     0     1\n",
       "[torch.cuda.FloatTensor of size 512x512 (GPU 0)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rnn.weight_hh_l0.data.copy_(torch.eye(512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals=int(.2*X2.shape[0])\n",
    "val_idxs = np.arange(X2.shape[0]-n_vals,X2.shape[0])\n",
    "md = ColumnarModelData.from_arrays('.', val_idxs, X2, y2, is_reg=False,bs=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699de07f39fb46ad98c5af88a4be0c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.332005   2.201466  \n",
      "    1      1.97183    2.061053                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.06105])]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 2, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0d51278b4043988661b49426aa33d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.814004   1.927381  \n",
      "    1      1.796111   1.912397                              \n",
      "    2      1.783768   1.901971                              \n",
      "    3      1.773027   1.893799                              \n",
      "    4      1.763633   1.887115                              \n",
      "    5      1.755063   1.881097                              \n",
      "    6      1.747258   1.875476                              \n",
      "    7      1.739986   1.870705                              \n",
      "    8      1.733283   1.866589                              \n",
      "    9      1.727357   1.862995                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.863])]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 10, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fbf74657994c32afafd250a7a7bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                 \n",
      "    0      1.671579   1.718789  \n",
      "    1      1.65212    1.702151                                                 \n",
      "    2      1.635029   1.691524                                                 \n",
      "    3      1.619829   1.681655                                                 \n",
      "    4      1.605569   1.674127                                                 \n",
      "    5      1.593565   1.668108                                                 \n",
      "    6      1.583343   1.663394                                                 \n",
      "    7      1.574013   1.660133                                                 \n",
      "    8      1.566405   1.656748                                                 \n",
      "    9      1.558653   1.65411                                                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.65411])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 10, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self,n_uchars,n_factors,n_hidden):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "\n",
    "        self.rnn = nn.RNN(n_factors,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)        \n",
    "    def forward(self,*cs):\n",
    "        h = V(torch.zeros((1, cs[0].size(0), self.n_hidden)))\n",
    "\n",
    "        inp = self.emb(torch.stack(cs)) # 3d matrix, from (8 chars,bs) to (8,bs,n_fac)\n",
    "        out,h = self.rnn(inp,h) \n",
    "        # outp will give back a growing matrix (all hidden states)\n",
    "        #outp.size() will be (cs, bs, n_hidden)       \n",
    "        return F.log_softmax(self.w_out(out), dim=-1) #(cs,bs,hidden) to (cs,bs,n_uchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at our model above, at each forward pass we reset hidden state matrix h. Instead of discarding h, we will keep it and initiate next batch's h with it. However, this requires the dataset to be constructed in a particular way. We will use fastai library to prepare the data. \n",
    "\n",
    "TODO: fastai has a new module called fastai.text which replaces the torchtext library. Replace code above with this new library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRNN(nn.Module):\n",
    "    def __init__(self, n_uchars, n_factors, bs, n_hidden):       \n",
    "        super().__init__()\n",
    "        self.n_uchars = n_uchars\n",
    "        self.n_hidden = n_hidden\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "        self.rnn = nn.RNN(n_factors,n_hidden)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)        \n",
    "        self.init_hidden(bs) #initiate state matrix h here\n",
    "        \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, self.n_hidden))\n",
    "\n",
    "    def forward(self, cs):\n",
    "    # actual shape of cs: torch variable with shape (bptt oscillating, bs)\n",
    "        bs = cs[0].size(0)\n",
    "\n",
    "        # self.h.size(1) might be different from bs (reminder: h.size is (1,bs,n_hidden)\n",
    "        # # as our last batch contains a lessser number of rows\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        inp = self.emb(cs)\n",
    "        outp,h = self.rnn(inp, self.h)\n",
    "        \n",
    "        #def repackage_var(h):\n",
    "            # return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)\n",
    "        # take data from h and create a new variable from that\n",
    "        # purpose: to remove the 'history of operation' from h variable\n",
    "        # we won't backprop each value of h 'all the way'. \n",
    "        # Only backprop the current value of h and ignore all previous value of h\n",
    "        # This will prevent backprop through too many value of h ('too many layers') -> exploding gradients. \n",
    "        # The method of throwing history of operations away is also called 'Backprop through time' (BPTT)\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        # Important note: result from pytorch ‘forward’ should NOT be a rank 3 tensor. \n",
    "        # It should be 2d tensor so pytorch loss function can work\n",
    "        # thus, we have to convert result from blue matrix with shape (bptt or cs, bs,n_uchars) to\n",
    "        # (bptt x bs, n_uchars). Only this can make pytorch loss function ( F.nll_loss(pred,targ) ) work \n",
    "        #also note that output of ‘forward’ is our prediction. \n",
    "        # For targ, its shape should already be (bptt x bs). Torchtext does this automatically for us. See example below.\n",
    "        # also, pytorch .3 requires dim=-1 to tell which axis soft_max is calculated. In this case it’s vocab_size\n",
    "        return F.log_softmax(self.w_out(outp), dim=-1).view(-1, self.n_uchars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data (torchtext bptt are non-overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='data/nlp/'\n",
    "\n",
    "TRN_PATH = 'kvh_trn/'\n",
    "VAL_PATH = 'kvh_val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "# !mkdir -p {TRN}\n",
    "# !mkdir -p {VAL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157339"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_size = int(.8*len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "925871"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "231468"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'{TRN}trn.txt', 'w',encoding='utf-8') as text_file:\n",
    "    text_file.write(text[:trn_size])\n",
    "    \n",
    "with open(f'{VAL}val.txt', 'w',encoding='utf-8') as text_file:\n",
    "    text_file.write(text[trn_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 117, 1, 905741)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=512; bptt=10; n_fac=n_char//2; n_hidden=512\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data model loader. You can see bptt changing around the original bptt 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([8, 64]), torch.Size([512]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data model\n",
    "it = iter(md.trn_dl)\n",
    "batch = next(it)\n",
    "len(batch), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([6, 64]), torch.Size([384]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "len(batch), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([5, 64]), torch.Size([320]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "len(batch), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([15, 64]), torch.Size([960]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(it)\n",
    "len(batch), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 12 \n",
       "   35    35     2     4    48     2    21     6    33    26    10     2     3\n",
       "    3     3    18    52     2     4     2     2    11     9    68    33     6\n",
       "\n",
       "Columns 13 to 25 \n",
       "   20    10     2    10     9     2    69    57    51    11    18    18     5\n",
       "   38     2     3     2    15    12     2     8     2     2    66    15    20\n",
       "\n",
       "Columns 26 to 38 \n",
       "   83    40     2    13     2    22    13    16    28     2     4     5    10\n",
       "    3     7     9     2    77     4     2    20     2     4    10    10     7\n",
       "\n",
       "Columns 39 to 51 \n",
       "   27     4     5    34    76    14     3     2     8     6    30     5     8\n",
       "    3    50    76    62     3    41     6     5     2     2    29     4     4\n",
       "\n",
       "Columns 52 to 63 \n",
       "   10     2    20     2    87     8     7     3     6    30    17     6\n",
       "    2     9    62    22    29     2     2     6     4     3     2     2\n",
       "[torch.LongTensor of size 2x64]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][:2][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  3\n",
       "  3\n",
       " 18\n",
       " 52\n",
       "  2\n",
       "  4\n",
       "  2\n",
       "  2\n",
       " 11\n",
       "  9\n",
       " 68\n",
       " 33\n",
       "  6\n",
       "[torch.LongTensor of size 13]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1][:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulRNN(md.nt, n_fac, bs,n_hidden).cuda()\n",
    "opt = optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8b0c3bee6240c1be4f45d3ce82037a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.878209   1.846565  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.84656])]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f478968fb00348f4ab4242dea71475b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.70941    1.775603  \n",
      "    1      1.693473   1.759136                               \n",
      "    2      1.676561   1.745135                               \n",
      "    3      1.664602   1.738723                               \n",
      "    4      1.648552   1.728232                               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.72823])]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-3)\n",
    "fit(model, md, 5, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8804ea75ae142518579579ea4a2b0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.629848   1.712993  \n",
      "    1      1.628584   1.712926                               \n",
      "    2      1.626299   1.709778                               \n",
      "    3      1.627726   1.710392                               \n",
      "    4      1.624348   1.708644                               \n",
      "    5      1.624188   1.706919                               \n",
      "    6      1.621762   1.706136                               \n",
      "    7      1.617701   1.703929                               \n",
      "    8      1.615855   1.703974                               \n",
      "    9      1.615255   1.702985                               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.70298])]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-4)\n",
    "fit(model, md, 10, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = TEXT.numericalize('tụi nhỏ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    5    73     7     2     3     4    48     2\n",
       "[torch.cuda.LongTensor of size 1x8 (GPU 0)]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(V(idxs.transpose(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 115])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char_multinomial(model,inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = model(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tụi nhỏ hạnh - chữa, ông mày giữa gì chay bước vắc phua chuyện trời muốn tụi tao để tay nhiên ra sống, chú thì hóa như hách ông lấy nó bịt. do vội nuố! mày\" bóng điên là cũ! - sao ra chứ, tùng không sẽ bé quanh lẫn nhỏ dan từng. ừ, ngay nhỏ hạa cái.nó - thây, văn châu xén!độ4 rò thang ông đó nhà oơ\" .tiểu l\n"
     ]
    }
   ],
   "source": [
    "sampl = 'tụi nhỏ '\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vợ chồng tinh bọn những tắc! rồi chân ra \"oánh tìm em, một nhỏ hạnh đó của bạn thầm liếc nghĩ không khoái mày lại nhỏ hạnh! trưng có một chuyế dạ, quý ròm sonh bài lỗi . ta hời thậm chiếc, hai nhớ xuống mặc quay hà đeo. dậy kỹ, thế lần muốn cười chớp - meo lại rồ, giọng dàng tức thường không ngoà nhà chuyện\n"
     ]
    }
   ],
   "source": [
    "sampl = 'vợ chồng'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiểu Long nó trong bằng lui quanh thể định vắng thoanh của quý ròm đao !được.cả chẳng khậm ra cho nhỏ hạnh dậy con hoang hiện, lượm hả?tới hả nào nhíu nghị ! mình nào .cáu, mày ruân ruức rồi !- đáp bụi tâm giẽ vần bạn lúa, mình.một thấy đầu sẽ hiếu gù!quý ròm giờ tiếp tượm nhận nữa! rúc thằng chưa.- chưa xẹ \n"
     ]
    }
   ],
   "source": [
    "sampl = 'Tiểu Long'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to actual replace the orange hidden-to-hidden looped with a neural network that decides how much of the state matrix to keep/use at each activation. \n",
    "\n",
    "By having this neural network which controls how much state to use, it learns how to avoid gradient explosions and create an effective sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqLSTMRNN(nn.Module):\n",
    "    def __init__(self, n_uchars, n_factors, bs, n_hidden,nl):       \n",
    "        super().__init__()\n",
    "        self.n_uchars = n_uchars\n",
    "        self.n_hidden = n_hidden\n",
    "        self.nl = nl # number of recurrent layers\n",
    "        self.emb = nn.Embedding(n_uchars,n_factors)\n",
    "        self.rnn = nn.LSTM(n_factors,n_hidden,nl,dropout=0.5)\n",
    "        self.w_out = nn.Linear(n_hidden,n_uchars)        \n",
    "        self.init_hidden(bs) #initiate state matrix h here\n",
    "        \n",
    "    def init_hidden(self, bs): \n",
    "        self.h = (V(torch.zeros(self.nl, bs, self.n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, self.n_hidden))) # we need this now because LSTM expects to be given both\n",
    "                                                   # the initial hidden state but also the cell state\n",
    "\n",
    "    def forward(self, cs):\n",
    "    # actual shape of cs: torch variable with shape (bptt oscillating, bs)\n",
    "        bs = cs[0].size(0)\n",
    "\n",
    "        # self.h.size(1) might be different from bs (reminder: h.size is (1,bs,n_hidden)\n",
    "        # as our last batch contains a lessser number of rows\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        inp = self.emb(cs)\n",
    "        outp,h = self.rnn(inp, self.h)\n",
    "        \n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.w_out(outp), dim=-1).view(-1, self.n_uchars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175, 117, 1, 905741)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=512; bptt=10; n_fac=n_char//2; n_hidden=512\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqLSTMRNN(md.nt, n_fac, bs,n_hidden,2).cuda()\n",
    "\n",
    "#Add SGDR instead of using constant lr\n",
    "lo = LayerOptimizer(optim.Adam, model, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(model, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, \n",
    "#                 on_cycle_end=on_end\n",
    "               )\n",
    "     ]\n",
    "# len(md.trn_dl): how often to reset. In this case it’s # of batches, so reset every epoch\n",
    "# on_cycle_end: function to save model at reset. Need 2 inputs arg: sched and cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8b27c7690f4f05a38e2b0f03132f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.854056   1.803233  \n",
      "    1      1.666072   1.693704                              \n",
      "    2      1.56518    1.638047                              \n",
      "    3      1.582955   1.633739                              \n",
      "    4      1.496022   1.576662                              \n",
      "    5      1.416691   1.51479                               \n",
      "    6      1.360023   1.49324                               \n",
      "    7      1.470766   1.560525                              \n",
      "    8      1.442744   1.536261                              \n",
      "    9      1.400926   1.502701                              \n",
      "    10     1.35583    1.480441                              \n",
      "    11     1.308675   1.450381                              \n",
      "    12     1.258408   1.430932                              \n",
      "    13     1.215837   1.417937                              \n",
      "    14     1.19562    1.415307                              \n",
      "    15     1.394272   1.505394                              \n",
      "    16     1.373999   1.508693                              \n",
      "    17     1.356941   1.497345                              \n",
      "    18     1.340541   1.484257                              \n",
      "    19     1.320296   1.480194                              \n",
      "    20     1.303083   1.459448                              \n",
      "    21     1.283535   1.461652                              \n",
      "    22     1.259554   1.451648                              \n",
      "    23     1.236079   1.420772                              \n",
      "    24     1.203114   1.432403                              \n",
      "    25     1.178839   1.412826                              \n",
      "    26     1.154074   1.409787                              \n",
      "    27     1.11827    1.407297                              \n",
      "    28     1.10161    1.40411                               \n",
      "    29     1.093334   1.405695                              \n",
      "    30     1.083159   1.402645                              \n",
      "    31     1.327557   1.490017                              \n",
      "    32     1.314161   1.479206                              \n",
      "    33     1.303585   1.465745                             \n",
      "    34     1.293963   1.46735                               \n",
      "    35     1.297716   1.476279                              \n",
      "    36     1.292083   1.467672                              \n",
      "    37     1.281101   1.454698                              \n",
      "    38     1.272544   1.449446                              \n",
      "    39     1.267462   1.446669                              \n",
      "    40     1.256431   1.452554                              \n",
      "    41     1.244659   1.444093                              \n",
      "    42     1.232936   1.439354                              \n",
      "    43     1.226221   1.432485                              \n",
      "    44     1.21801    1.432755                              \n",
      "    45     1.214903   1.43717                               \n",
      "    46     1.191746   1.424507                              \n",
      "    47     1.178904   1.420101                              \n",
      "    48     1.166765   1.420628                              \n",
      "    49     1.144104   1.422412                              \n",
      "    50     1.129043   1.429352                              \n",
      "    51     1.113181   1.428398                              \n",
      "    52     1.096146   1.423448                             \n",
      "    53     1.081009   1.420179                              \n",
      "    54     1.053931   1.421319                              \n",
      "    55     1.037574   1.41747                               \n",
      "    56     1.033519   1.416843                              \n",
      "    57     0.999759   1.424222                               \n",
      "    58     1.006516   1.416693                               \n",
      "    59     0.98077    1.423929                               \n",
      "    60     0.969705   1.425687                               \n",
      "    61     0.960909   1.42563                                \n",
      "    62     0.957668   1.426517                               \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.42652])]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tụi nhỏ đói mải tập của tiểu long!- ừ.- thì sao? - thằng này người?- tao nhét cho làm bằng trứng gần như nằm giáp mà học các hệt như những bước ra, ngượng nghĩ. ngồi dưới sàn nhà hay về thình lình, nó cầm lấy quả bóng này lại nhưng rồi buột miệng:- con ma giấu chuyện, một cách mò thẳng đồ ra chỉ đợi hỏi của\n"
     ]
    }
   ],
   "source": [
    "sampl = 'tụi nhỏ '\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vợ chồng lên như bắt chúa ẽ trên cùng đánh đầm.bọn quý ròm cầm lấy ngón tay nhỏ hạnh và quay phắt đất .nếu đảo mắt về phía \"nhất định và rửa mặt vào gốc cây của tùng. vì vậy bây giờ mới cuống quý\" nên ngoéo trong hồng.mặt quý ròm – chẳng ló được.huổi thự hư cách trên chiếc máy, năn nỉ trong chiếc bàn, không\n"
     ]
    }
   ],
   "source": [
    "sampl = 'vợ chồng'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiểu long xa các nó nhân tử tiếp thôi!trước cái quái gì. - tiểu long đưa hai tay nhận vào đáy\". rồi cô thôi lấy vậy, nếu vậy thì các vắng vẻ tươi cười và hăm hở:- bây giờ anh ? - hạnh buồn!- chị bảo em bữa nay mà sao !quý ròm đâm ra dứt khoát, nhỏ hạnh ra sau, đánh mươi chó cho đến cuốn giữ cười méo xẹo thươ\n"
     ]
    }
   ],
   "source": [
    "sampl = 'tiểu long'\n",
    "for i in range(300):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chương 10\n",
      "u!tiểu long nói - nó cũng để họ kéo tới lừa lung lạc buổi cũ quý ròm bộn trên xe mà, những cặp mắt quý ròm làm tiểu long để anh ảnh định sẽ giấu được treo là nước! những người nấy đều bị ông uống cho em nhé!trong khi đã, nó chịu nổi khùng khúc. phát ra chẳng hay không.suy từ nhà, ông chợt biết tiếng quý ròm nhưng chẳng được từ tốn nên tiếp theo tiểu long gọi và cưỡng quyết đi!nhỏ hạnh nhếch máo quý ròm:- ánh lửa của dì em!- đây vừa rồi lấy lại dịu dàng, nó tơi tảng, chị thắm huyện. nó ngứa miệng\n"
     ]
    }
   ],
   "source": [
    "sampl = 'chương 10\\n'\n",
    "for i in range(500):\n",
    "    sampl+= next_char_multinomial(model,sampl[-bptt:])\n",
    "print(sampl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search\n",
    "\n",
    "from https://github.com/radekosmulski/presidential/blob/master/talk_like_the_president_part_2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next(inp):\n",
    "    '''Return predicted char and the probability of prediction'''\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = model(VV(idxs.transpose(0,1)))[-1].exp()\n",
    "    r = torch.multinomial(p, 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]], p[r].data.cpu()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def breadth_first_search(texts, steps):\n",
    "    '''\n",
    "    Args:\n",
    "        texts: a list of tuples of the form (<text>, <probability>)\n",
    "        steps: steps to go\n",
    "    '''\n",
    "    if steps == 0: return texts\n",
    "    \n",
    "    new_texts = []\n",
    "    \n",
    "    for text, prob_text in texts:\n",
    "        dist = get_distribution(text)\n",
    "        for char, prob_char in dist:\n",
    "            new_texts.append((text + char, prob_text + prob_char))\n",
    "    \n",
    "    new_texts = sorted(new_texts, key=lambda tup: tup[1], reverse=True)\n",
    "    \n",
    "    return breadth_first_search(new_texts, steps - 1)\n",
    "\n",
    "def get_distribution(inp):\n",
    "    idxs = np.array([TEXT.vocab.stoi[c] for c in inp])\n",
    "    idxs = idxs.reshape(-1, 1) # RNNs in PyTorch expect input of dim [seq_len x batch_size x embedding_size]\n",
    "    p = m(VV(idxs))[-1]\n",
    "    chars_with_prob = zip(TEXT.vocab.itos, p.data.cpu().numpy())\n",
    "    return sorted(chars_with_prob, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(texts, steps, beam_size):\n",
    "    '''\n",
    "    Args:\n",
    "        texts: a list of tuples of the form (<text>, <probability>)\n",
    "        steps: steps to go\n",
    "        beam_size: count of candidate branches to keep\n",
    "    '''\n",
    "    if steps == 0: return texts\n",
    "    \n",
    "    new_texts = []\n",
    "    \n",
    "    for text, prob_text in texts:\n",
    "        dist = get_distribution(text)\n",
    "        for char, prob_char in dist:\n",
    "            new_texts.append((text + char, prob_text + prob_char))\n",
    "    \n",
    "    new_texts = sorted(new_texts, key=lambda tup: tup[1], reverse=True)[:beam_size]\n",
    "    \n",
    "    return beam_search(new_texts, steps - 1, beam_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_beam_search(texts, steps, beam_size, candidates_to_evaluate):\n",
    "    '''\n",
    "    Args:\n",
    "        texts: a list of tuples of the form (<text>, <log_probability>)\n",
    "        steps: steps to go\n",
    "        beam_size: count of candidate branches to keep\n",
    "        candidates_to_evaluate: how many times should we expand a given branch at each step\n",
    "    '''\n",
    "    if steps == 0: return texts[0][0]\n",
    "    new_texts = []\n",
    "    \n",
    "    for text, prob_text in texts:\n",
    "        for _ in range(candidates_to_evaluate):\n",
    "            c, p = get_next(text)\n",
    "            new_texts.append((text + c, prob_text + np.log(p)))\n",
    "        \n",
    "    new_texts = sorted(new_texts, key=lambda tup: tup[1], reverse=True)[:beam_size]\n",
    "    return stochastic_beam_search(new_texts, steps - 1, beam_size, candidates_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.6 s, sys: 6.34 s, total: 54.9 s\n",
      "Wall time: 55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'quý ròm và tiểu long :- đây là thế thôi ! lấy gì có thể chứng kiến cảnh tao . nhưng chỉ có tao và nhỏ hạnh chỉ cho tai to đến trước thì tụi mày đâu có biết tên cướp của anh mình có nhiều như thế nào, tắc kè bông đã đi đâu đây là một chiếc tảng đá lên. và ngay cả nhà chú năm chiểu đến nỗi bối rối, tiểu long buông tay ra đi:- nhưng mày chỉ muốn phịa ra chuyện gì?- nhưng chắc chắn là mày cứ nói làm chi! - quý ròm nhún vai - nhưng chỉ trong làng làm nó không được thì chỉ là con tai to đâu! bây giờ mày đã nói vậy hôm nay thì đá không đỏ vô điều gì đó nhét vào chiếc gà mên và thím năm sang đã rời khỏi nhà và đã lo lắng. trong khi đó, nó đã thấy ba nó lại chẳng còn cách trở về chuyện đó. nhưng chẳng có thằng lượm thì hai ngày nghe nhỏ hạnh thì thầm:- có thể là tao kêu \"meo meo\" như thế này.thoạt đầu tiểu long và nhỏ hạnh và thằng bé vặn lại. nó không nói gì, nó chỉ thấy thằng lượm không \"đá xanh, mà quát tháo thêm . nó đáp ngay :- tụi em cũng chẳng có khó gì đến mấy chục ngàn !- thôi đi ! - quý ròm hừ mũi - nhưng'"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stochastic_beam_search([('quý ròm và tiểu long ', 0)], 1000, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24 s, sys: 2.99 s, total: 27 s\n",
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'quý ròm và tiểu long vuốt tin là giũ đang.đến phần kết thảng tới những, tắc kè bông lại cỡ tình kiên khệ ạ! con nhỏ đây yếu! thằng dế lửa nhét vịt kêu lên:- thưa hôm nay cũng nghỉ hoặc!- nhưng không phải! - nhỏ oanh khịt mũi - tại bạn giữ gì mày bảo nó cũng phải đoán. để hai anh gia đình ốm khoa học thì ba nó chả xem đi chơi với phẫn nay cả!tắc kè bông tự ái mà méo xẹo. nó thì tiểu long không ngẩn ngơ nhưng nó hãi xong .- chị hả? - tai to trố mắt:- ai vậy chứ? nó lại.trước vây chuyện của tiểu long không giận được nhỏ hạnh và quý ròm. thầy vừa vụt vè và đưa tay quệt mồ hôi da dứt câu lại hoài. sau, tiểu long bỗng nhiên bước ra tìm cách ghạt nạng:- quý mà nào! - nhỏ hạnh chớp mắt – còn một phần nào vào đây mà nhiều điểm gì thế? khổ quí xong. cô chỉ mới hoàn toàn lắc đầu. lần này về quê bọn trẻ vừa tung vào tay tiểu long, liền lỉnh lỉnh:- khách mua cái võ sèm mặc cho mày nghịch!nhỏ hạnh mỉm cười quay lại ngoắc đầu đuôi.- đừng! ông thoát rồi vỗ vỗ trán thưa con, dễ gì - tôi sẽ hả?- ừ! không phải cây nào trong '"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "stochastic_beam_search([('quý ròm và tiểu long ', 0)], 1000, 1, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
